{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haw River Levels\n",
    "Data feed from [USGS REST API](https://waterservices.usgs.gov/rest/IV-Service.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bqplot as bq\n",
    "import ipyleaflet as lf\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from bqplot import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://waterservices.usgs.gov/nwis/iv/?format=json' + \\\n",
    "        '&sites=02096960,02096500' + \\\n",
    "        '&startDT=2018-08-01' + \\\n",
    "        '&endDT=2018-10-01' + \\\n",
    "        '&parameterCd=00060,00065' + \\\n",
    "        '&siteStatus=all'\n",
    "\n",
    "def get_location_data(location):\n",
    "    '''Extracts location data from a timeSeries data array.\n",
    "    Returns a dictionary of site data and a dataframe of\n",
    "    the metric for that location.'''\n",
    "    \n",
    "    # Time series array to time series dataframe\n",
    "    metrics = pd.DataFrame.from_records(location['values'][0]['value'])\n",
    "    \n",
    "    # Set data types\n",
    "    metrics['value'] = metrics['value'].astype('float64')\n",
    "    metrics['dateTime'] = metrics['dateTime'].astype('datetime64')\n",
    "    \n",
    "    # Add time limits to meta data\n",
    "    start = metrics['dateTime'].min()\n",
    "    end = metrics['dateTime'].max()\n",
    "    \n",
    "    # Set value name to metric description\n",
    "    description = location['variable']['variableDescription']\n",
    "    description = description.replace(' ','_').replace(',','').lower()\n",
    "    metrics.rename(columns={'value':description}, inplace=True)\n",
    "    \n",
    "    # Set index to timescale\n",
    "    metrics.set_index('dateTime', inplace=True)\n",
    "    \n",
    "    # Change array in qualifiers to string\n",
    "    if type(metrics.qualifiers[0]) == list:\n",
    "        \n",
    "        metrics['status'] = [i[0] for i in metrics.qualifiers]\n",
    "    \n",
    "    # Dictionary of metadata with metrics dataframe\n",
    "    source = location['sourceInfo']\n",
    "    geo_info = source['geoLocation']['geogLocation']\n",
    "    \n",
    "    site_data = {\n",
    "        'site_name':source['siteName'].title().replace('Nc','NC'),\n",
    "        'site_code':source['siteCode'][0]['value'],\n",
    "        'network':source['siteCode'][0]['network'],\n",
    "        'projection':geo_info['srs'],\n",
    "        'latitude':geo_info['latitude'],\n",
    "        'longitude':geo_info['longitude'],\n",
    "        'coordinates':(geo_info['latitude'],\n",
    "                       geo_info['longitude']),\n",
    "        'measurement':location['variable']['variableName'],\n",
    "        'description':description,\n",
    "        'start_datetime': start,\n",
    "        'end_datetime': end,\n",
    "        'metrics':metrics,\n",
    "                }\n",
    "    \n",
    "    \n",
    "    return (site_data)\n",
    "\n",
    "\n",
    "def join_metrics(previous, current):\n",
    "    '''Inner join dataframes. \n",
    "    Removes duplicate columns to reduce data size. \n",
    "    Returns merged dataframe. '''\n",
    "            \n",
    "    # Get the columns from each set\n",
    "    current_columns = set(current.columns.tolist())\n",
    "    prev_columns = set(previous.columns.tolist())\n",
    "\n",
    "    # Find the duplicates\n",
    "    dupes = list(current_columns.intersection(prev_columns))\n",
    "\n",
    "    # Inner join the datasets, drop duplicate columns\n",
    "    merged = pd.merge(current.drop(\n",
    "                        columns=['qualifiers',]),\n",
    "              previous.drop(\n",
    "                        columns=dupes),\n",
    "              left_index = True,\n",
    "              right_index = True)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def combine_location_metrics(dataset):\n",
    "    '''Joins data based on location.'''\n",
    "    \n",
    "    prev_location = [None,]\n",
    "    prev_key = None\n",
    "    location_datasets = {}\n",
    "    \n",
    "    for key in dataset.keys():\n",
    "        # Extract the location portion of the name\n",
    "        location = key.split('-')[0]\n",
    "\n",
    "        # Match against previous location\n",
    "        if prev_location[0] == location:\n",
    "\n",
    "            # Pull the dataframes\n",
    "            current = dataset[key]['metrics']\n",
    "            previous = dataset[prev_key]['metrics']\n",
    "\n",
    "            # Combine the dataframes\n",
    "            merged = join_metrics(previous, current)\n",
    "\n",
    "            # Add the combined data to the datasets\n",
    "            location_datasets[location] = merged\n",
    "\n",
    "        # Set the previous data variables for the next iteration\n",
    "        prev_location = [location,]\n",
    "        prev_key = key\n",
    "        \n",
    "    return location_datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data downloaded\n",
      "Data Parsed\n"
     ]
    }
   ],
   "source": [
    "# Future Main Loop\n",
    "\n",
    "data = requests.get(url)\n",
    "if data.status_code != 200:\n",
    "    print('Server returned ' + data.status_code + ' exiting...')\n",
    "    end\n",
    "        \n",
    "print('data downloaded')\n",
    "    \n",
    "try:\n",
    "    raw_data = data.json()\n",
    "    print('Data Parsed')\n",
    "except JSONDecodeError:\n",
    "    print('Request Timed Out')\n",
    "\n",
    "time_series = raw_data['value']['timeSeries']\n",
    "\n",
    "dataset = {}\n",
    "for location in time_series:\n",
    "    site_data = get_location_data(location) \n",
    "    #print(site_data['metrics'])\n",
    "    unique_name = site_data['site_name'] + \\\n",
    "    '-' + \\\n",
    "    site_data['description']\n",
    "    key_name = unique_name.lower().replace(' ','_').replace(',','')\n",
    "    dataset[key_name] = site_data\n",
    "\n",
    "location_data = combine_location_metrics(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e99bf18a59e4a038e8526580c3ed781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(axes=[Axis(scale=DateScale()), Axis(orientation='vertical', scale=LinearScale())], fig_margin={'top': 6â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# future plot function\n",
    "\n",
    "# Extract from the data frame\n",
    "fig_title = list(location_data.keys())[0]\n",
    "df = location_data[fig_title]\n",
    "date = df.index.values\n",
    "depth = list(df.gage_height_feet.replace(-999999.0, 0))\n",
    "flow = list(df.discharge_cubic_feet_per_second.replace(-999999.0, 0))\n",
    "\n",
    "# Set up the axis\n",
    "dt_x = bq.DateScale()\n",
    "sc_y = bq.LinearScale()\n",
    "ax_x = bq.Axis(scale=dt_x)\n",
    "ax_y = bq.Axis(scale=sc_y, orientation='vertical')\n",
    "\n",
    "line = bq.Lines(x=date, y=depth, scales={'x': dt_x, 'y': sc_y})\n",
    "\n",
    "plt.figure(marks=[line], axes=[ax_x, ax_y], title=fig_title)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
